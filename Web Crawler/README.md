# Web Crawler

## ğŸ“Œ Overview
This project implements a **web crawler** in Java using the `crawler4j` framework.  
It crawls the **USAToday** website (or any given seed URL), collects pages, and reports statistics such as the number of fetched, successful, and failed URLs.

---

## âš™ï¸ Technologies Used
- Java  
- crawler4j framework  
- Eclipse IDE  
- JSON, CSV for data storage  

---

## ğŸ“ Folder Structure
```
Web Crawler/
â”‚
â”œâ”€â”€ .settings/           # Eclipse settings
â”œâ”€â”€ bin/                 # Compiled Java class files
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ crawl/           # Raw crawled data
â”‚   â”œâ”€â”€ output/          # Output logs and reports
â”‚   â””â”€â”€ report/          # Crawl summaries
â”œâ”€â”€ lib/                 # External libraries (crawler4j, jsoup)
â”œâ”€â”€ reference/           # Reference material and configs
â”œâ”€â”€ src/                 # Java source code
â”‚
â”œâ”€â”€ CrawlReport_usatoday # Crawl summary output
â”œâ”€â”€ readme               # Project documentation
â””â”€â”€ .project / .classpath
```

---

## ğŸš€ How to Run
1. Open the project in **Eclipse**.
2. Add crawler4j and jsoup JARs from the `/lib` folder to your build path.
3. Edit the main class to set the desired seed URL and crawl depth.
4. Run the program â€” it will generate:
   - Crawled pages in `/data/crawl`
   - Log files in `/data/output`
   - Summary report `CrawlReport_usatoday`

---

## âš ï¸ Note
The large CSV file `urls_USAToday.csv` (â‰ˆ176 MB) was removed from the repository due to GitHub file limits.  
If required, it can be regenerated by re-running the crawler with the same configuration.

---

## ğŸ“Š Output Summary
- Total URLs fetched  
- Successful vs failed fetches  
- File size statistics  
- Unique vs duplicate URLs  
- Crawl duration and status codes
