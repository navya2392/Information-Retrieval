# Web Crawler

## 📌 Overview
This project implements a **web crawler** in Java using the `crawler4j` framework.  
It crawls the **USAToday** website (or any given seed URL), collects pages, and reports statistics such as the number of fetched, successful, and failed URLs.

---

## ⚙️ Technologies Used
- **Language:** Java  
- **Framework:** crawler4j 4.4.0  
- **Libraries:** SLF4J, Logback, jsoup  
- **IDE:** Eclipse  
- **Outputs:** CSV files for fetched pages, visits, and URLs 

---

## 📁 Folder Structure
```
Web Crawler/
│
├── .settings/           # Eclipse settings
├── bin/                 # Compiled Java class files
├── data/
│   ├── crawl/           # Raw crawled data
│   ├── output/          # Output logs and reports
│   └── report/          # Crawl summaries
├── lib/                 # External libraries (crawler4j, jsoup)
├── reference/           # Reference material and configs
├── src/                 # Java source code
│
├── **CrawlReport_usatoday** # Crawl summary output
├── readme               # Project documentation
└── .project / .classpath
```

## 🚀 How to Run
1. Open the project in **Eclipse**.
2. Add crawler4j and jsoup JARs from the `/lib` folder to your build path.
3. Edit the main class to set the desired seed URL and crawl depth.
4. Run the program — it will generate:
   - Crawled pages in `/data/crawl`
   - Log files in `/data/output`
   - Summary report `CrawlReport_usatoday`

---
## 🔄 Process Used
1. **Controller Setup**  
   The `Controller.java` configures crawl parameters — 7 threads, 10,000 max pages, depth 16, and 1500 ms politeness delay. It initializes `CrawlConfig`, `PageFetcher`, and `RobotstxtServer`, then starts crawling from `https://www.usatoday.com/`.
2. **Crawling and Filtering**  
   `MyCrawler.java` handles link filtering and page visits. Only `http` and `https` URLs under `usatoday.com` are followed, excluding `.css`, `.js`, `.zip`, `.json`, `.gz`, etc. Subdomains like “www.usatoday.com” are included, while external sites are marked as `N_OK`.
3. **Logging and CSV Generation**  
   - `fetch_usatoday.csv`: records each fetch attempt with HTTP status.  
   - `visit_usatoday.csv`: logs visited pages with file size, content type, and outlink count.  
   - `urls_usatoday.csv`: tracks all discovered links (`OK` or `N_OK`).  
   Redirects are captured (3xx → target URL).
4. **Post-Processing**  
   The crawler compiles aggregate metrics into `CrawlReport_usatoday.txt` — summarizing fetch success rates, content size distributions, and MIME-type counts.

---
## 📊 Output Summary
**Crawl Parameters:**  
- News Site: `usatoday.com`  
- Threads: 7  
- Max Fetches: 10,000  

**Fetch Statistics:**  
- Total attempted: **10,000**  
- Succeeded: **9,795**  
- Failed or aborted: **205**

**Outgoing URLs:**  
- Total extracted: **2,080,427**  
- Unique URLs: **1,098,563**  
- Within domain: **981,799**  
- Outside domain: **116,764**

**Status Codes:**  
- 200 (OK): 9,795  
- 301/302 (Redirects): 176  
- 404/410 (Not Found/Gone): 22  
- 500 (Server Error): 1  

**File Sizes:**  
- `<1 KB:` 7  
- `1–10 KB:` 662  
- `10–100 KB:` 2,225  
- `100 KB–1 MB:` 6,634  
- `≥1 MB:` 101  

**Content Types:**  
- text/html: 8,237  
- image/jpeg: 1,063  
- image/png/webp/svg/x-icon: 329 


## ⚠️ Note
The large CSV file `urls_USAToday.csv` (≈176 MB) was removed from the repository due to GitHub file limits.  
If required, it can be regenerated by re-running the crawler with the same configuration.

---


