# Web Crawler

## 📌 Overview
This project implements a **web crawler** in Java using the `crawler4j` framework.  
It crawls the **USAToday** website (or any given seed URL), collects pages, and reports statistics such as the number of fetched, successful, and failed URLs.

---

## ⚙️ Technologies Used
- Java  
- crawler4j framework  
- Eclipse IDE  
- JSON, CSV for data storage  

---

## 📁 Folder Structure
```
Web Crawler/
│
├── .settings/           # Eclipse settings
├── bin/                 # Compiled Java class files
├── data/
│   ├── crawl/           # Raw crawled data
│   ├── output/          # Output logs and reports
│   └── report/          # Crawl summaries
├── lib/                 # External libraries (crawler4j, jsoup)
├── reference/           # Reference material and configs
├── src/                 # Java source code
│
├── CrawlReport_usatoday # Crawl summary output
├── readme               # Project documentation
└── .project / .classpath
```

---

## 🚀 How to Run
1. Open the project in **Eclipse**.
2. Add crawler4j and jsoup JARs from the `/lib` folder to your build path.
3. Edit the main class to set the desired seed URL and crawl depth.
4. Run the program — it will generate:
   - Crawled pages in `/data/crawl`
   - Log files in `/data/output`
   - Summary report `CrawlReport_usatoday`

---

## ⚠️ Note
The large CSV file `urls_USAToday.csv` (≈176 MB) was removed from the repository due to GitHub file limits.  
If required, it can be regenerated by re-running the crawler with the same configuration.

---

## 📊 Output Summary
- Total URLs fetched  
- Successful vs failed fetches  
- File size statistics  
- Unique vs duplicate URLs  
- Crawl duration and status codes
