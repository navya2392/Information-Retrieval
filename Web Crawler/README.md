# Web Crawler

## ğŸ“Œ Overview
This project implements a **web crawler** in Java using the `crawler4j` framework.  
It crawls the **USAToday** website (or any given seed URL), collects pages, and reports statistics such as the number of fetched, successful, and failed URLs.

---

## âš™ï¸ Technologies Used
- **Language:** Java  
- **Framework:** crawler4j 4.4.0  
- **Libraries:** SLF4J, Logback, jsoup  
- **IDE:** Eclipse  
- **Outputs:** CSV files for fetched pages, visits, and URLs 

---

## ğŸ“ Folder Structure
```
Web Crawler/
â”‚
â”œâ”€â”€ .settings/           # Eclipse settings
â”œâ”€â”€ bin/                 # Compiled Java class files
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ crawl/           # Raw crawled data
â”‚   â”œâ”€â”€ output/          # Output logs and reports
â”‚   â””â”€â”€ report/          # Crawl summaries
â”œâ”€â”€ lib/                 # External libraries (crawler4j, jsoup)
â”œâ”€â”€ reference/           # Reference material and configs
â”œâ”€â”€ src/                 # Java source code
â”‚
â”œâ”€â”€ **CrawlReport_usatoday** # Crawl summary output
â”œâ”€â”€ readme               # Project documentation
â””â”€â”€ .project / .classpath
```

## ğŸš€ How to Run
1. Open the project in **Eclipse**.
2. Add crawler4j and jsoup JARs from the `/lib` folder to your build path.
3. Edit the main class to set the desired seed URL and crawl depth.
4. Run the program â€” it will generate:
   - Crawled pages in `/data/crawl`
   - Log files in `/data/output`
   - Summary report `CrawlReport_usatoday`

---
## ğŸ”„ Process Used
1. **Controller Setup**  
   The `Controller.java` configures crawl parameters â€” 7 threads, 10,000 max pages, depth 16, and 1500 ms politeness delay. It initializes `CrawlConfig`, `PageFetcher`, and `RobotstxtServer`, then starts crawling from `https://www.usatoday.com/`.
2. **Crawling and Filtering**  
   `MyCrawler.java` handles link filtering and page visits. Only `http` and `https` URLs under `usatoday.com` are followed, excluding `.css`, `.js`, `.zip`, `.json`, `.gz`, etc. Subdomains like â€œwww.usatoday.comâ€ are included, while external sites are marked as `N_OK`.
3. **Logging and CSV Generation**  
   - `fetch_usatoday.csv`: records each fetch attempt with HTTP status.  
   - `visit_usatoday.csv`: logs visited pages with file size, content type, and outlink count.  
   - `urls_usatoday.csv`: tracks all discovered links (`OK` or `N_OK`).  
   Redirects are captured (3xx â†’ target URL).
4. **Post-Processing**  
   The crawler compiles aggregate metrics into `CrawlReport_usatoday.txt` â€” summarizing fetch success rates, content size distributions, and MIME-type counts.

---
## ğŸ“Š Output Summary
**Crawl Parameters:**  
- News Site: `usatoday.com`  
- Threads: 7  
- Max Fetches: 10,000  

**Fetch Statistics:**  
- Total attempted: **10,000**  
- Succeeded: **9,795**  
- Failed or aborted: **205**

**Outgoing URLs:**  
- Total extracted: **2,080,427**  
- Unique URLs: **1,098,563**  
- Within domain: **981,799**  
- Outside domain: **116,764**

**Status Codes:**  
- 200 (OK): 9,795  
- 301/302 (Redirects): 176  
- 404/410 (Not Found/Gone): 22  
- 500 (Server Error): 1  

**File Sizes:**  
- `<1 KB:` 7  
- `1â€“10 KB:` 662  
- `10â€“100 KB:` 2,225  
- `100 KBâ€“1 MB:` 6,634  
- `â‰¥1 MB:` 101  

**Content Types:**  
- text/html: 8,237  
- image/jpeg: 1,063  
- image/png/webp/svg/x-icon: 329 


## âš ï¸ Note
The large CSV file `urls_USAToday.csv` (â‰ˆ176 MB) was removed from the repository due to GitHub file limits.  
If required, it can be regenerated by re-running the crawler with the same configuration.

---


